---
title: "Qini curves: Automatic cost-benefit analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{maq}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
set.seed(123)
options(digits = 2)
```

```{r setup}
library(grf)
library(maq)
```
This vignette gives a brief overview of how Qini curves (or cost curves) can act as an attractive and intuitive metric for evaluating treatment rules when there are costs associated with deploying treatment, as well as how they can be generalized to many treatment arms, as implemented in the companion package [maq](https://github.com/grf-labs/maq). For complete details, we refer to [this paper](https://arxiv.org/abs/2306.11979).

## CATE evaluation as a policy evaluation exercise
Before jumping into Qini curves, let’s start by defining some terminology and refreshing some concepts. Consider a binary treatment assignment $W_i = \{0, 1\}$ and some outcome of interest $Y_i$. In order to determine if there are certain subgroups of the population, as defined by some observable characteristics $X_i$, that benefit differently from the treatment assignment, a central object of interest is the conditional average treatment effect (CATE)

 $$\tau(X_i) = E[Y_i(1) - Y_i(0) \,|\, X_i = x],$$

where $Y(1)$ and $Y(0)$ are potential outcomes corresponding to the two treatment states: control or treatment arm.

There are many approaches to obtain estimates of the function $\tau(X_i)$, *Causal Forest* being one of them. Now, once we an estimated $\hat \tau(\cdot)$ function, or set of functions, what *metric* can we use to evaluate them with? Recall that, as opposed to a classical prediction problem, we never observe ground truth treatment effects, so we cannot use a held-out test sample to compute something like a mean squared prediction error $E[(f(X_i) - Y_i)^2]$.

A metric we propose for this purpose is called the [RATE](https://grf-labs.github.io/grf/reference/rank_average_treatment_effect.html) and is covered in [this vignette](https://grf-labs.github.io/grf/articles/rate.html). With RATE we take a *policy evaluation* approach to guide the construction of a metric: assume we have obtained an estimated CATE function $\hat \tau(\cdot)$ on some *training set* and on a held-out *test set* $X_{test}$ we have observed outcomes $Y_i(W_i)$ for people who were treated, or not treated.

The estimated CATE function $\hat \tau(\cdot)$ induces a family of *policies*, which refer to as $\hat \pi(\cdot)$, that takes covariates $X_i$ and maps them to a treatment decision $\{0: \text{don't treat}, 1: \text{treat}\}$. For example, on the held-out test set, the predictions $\hat \tau(X_{test})$ implicitly tell us that a reasonable policy to determine treatment allocation with is: "If the estimated CATE for Alice is highest, then treat Alice first”, and "If the estimated CATE for Bob is the second highest, then treat Bob second", and so on.

The policy we just described can be more aptly termed a *prioritization rule*. The estimated CATEs implicitly tell us how to *prioritize* treatment allocation on a test set by following a simple *rule*: First treat Alice, then Bob, and so on, in order of decreasing CATE estimates. Recall that we have access to Bob and Alice’s observed outcomes $Y_i(W_i)$ on the test set, so we can evaluate the quality of this "predicted" policy by appropriately calculating some measure of agreement between $\hat \tau(X_{test})$ and $Y_i(W_i)$, i.e., do the people our CATE estimator give high priority to also have high average treatment effects as (appropriately) measured by their observed outcomes $Y_i(W_i)$? (for the purpose of this simple vignette we are assuming the treatment is randomly assigned, so that we can compute average treatment effects as simple differences in observed outcomes, the next section gives more detail, for complete details we refer to the papers listed in the references).

A first ingredient of RATE is to essentially take the estimated $\hat \tau(\cdot)$, treat it as a prioritization rule, then on the test set trace out the estimated average treatment effect (ATE) of people included in the rule minus the whole sample ATE, as we descend down the rule list "Alice, Bob, etc". We refer to this curve as the *TOC* curve. As mentioned in the [RATE](https://grf-labs.github.io/grf/articles/rate.html) vignette this is a visually appealing way to assess how a CATE estimator performs on a held-out test set. A second ingredient of a RATE is then to collapse this curve to a single point estimate, via computing an area under the curve (AUC), similar to how the area under the "ROC" curve can be used to assess a binary classifier.

This approach to evaluation can essentially be summarized as follows:

* A CATE estimator (causal forest, neural network, etc.) gives you an estimated function $\hat \tau (\cdot)$.
* This CATE function "induces" a policy $\hat \pi$ that you can evaluate on a test set by plotting a TOC curve.
* The RATE is a metric that can be used to quantify the value of this policy on a test set.

The appeal of this construction is that it enables you to transparently answer questions like "Did my estimated CATE function manage to detect treatment effect heterogeneity", or "Which of these estimated CATE functions performs best" - by conducting simple evaluation exercises on a held-out test set.


## Evaluation metrics when treatment assignment is costly
Consider now the case where assigning treatment incurs a cost, where we let $C_i(1)$ denote the cost of assigning unit $i$ the treatment arm (and assume that withholding treatment is costless, $C_i(0)=0$). An example where costs may vary by unit could be in administering vaccines: getting medicines to people who live very far away from healthcare centers is likely more costly than getting medicines to people who live close by. Costs does not have to be monetary, they could also capture something more abstract, such as negative externalities.

The question we now ask is, given a budget $B$, what is a suitable approach to quantify the cost-benefit tradeoff of assigning treatment in accordance with our estimated CATEs? It turns out that incorporating costs into the policy evaluation framework we outlined in the previous section is straightforward - but the curve is going to capture something different than the TOC.

Recall the policy $\hat \pi(X_i)$ is a function that maps covariates $X_i$ to a treatment decision. In this section, this function will depend on the budget $B$ which we denote by the subscript $\hat \pi_B(X_i)$. It turns out that this policy can be expressed, just as in the previous section, as a treatment prioritization rule that essentially says “If you have $b$ available to spend, then treat Alice first if her estimated cost-benefit ratio, CATE/cost is the highest”, and so on.

Just as before, we have available a test set with $n$ observed outcomes to perform evaluation on. We are interested in quantifying the expected *gain* (measured by the ATE) we can achieve by assigning treatment in accordance with $\hat \pi_B$ at different *spend* levels $B$.

In the previous section we left out the exact details of how to *evaluate* a policy. Luckily, it turns out this is simple: if we know the treatment randomization probabilities, then we can use inverse-propensity weighting (IPW) to estimate the value of the gain we achieve through averaging the difference in test set outcomes that matches our policy prescription:

$$
\frac{1}{n} \sum_{i}^{n} \hat \pi_B(X_i) \left( \frac{W_i Y_i}{P[W_i = 1|X_i]} - \frac{(1-W_i)Y_i}{P[W_i = 0|X_i]} \right).
$$
IPW (the terms in parenthesis) accounts for the fact that the prescribed policy $\hat \pi_B(X_i)$ might not match the observed treatment $W_i$ for unit i.

The *Qini curve* traces out the above estimated value, as we increase the budget. The following code example gives a toy example, where we to keep the exposition simple, assume each unit has the same cost, assigning treatment to both Bob and Alice costs 1.0, on some chosen denomination (a nice property of the Qini as an evaluation metric is that it does not require costs and treatment effects to be denominated on the same scale, only their ratio matters[^c]):

```{r}
n <- 2000
p <- 5
X <- matrix(rnorm(n * p), n, p)
W <- rbinom(n, 1, 0.5)
Y <- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)

# 1) Train a CATE function on training set.
train <- sample(n/2)
c.forest <- causal_forest(X[train, ], Y[train], W[train])

# 2) Predict CATEs on test set.
test <- -train
tau.hat <- predict(c.forest, X[test, ])$predictions

# 3) Form a Qini curve with inverse-propensity weighted test set outcomes.
Y.ipw.test <- ifelse(W[test] == 1, Y[test]/0.5, -Y[test]/0.5)
cost <- 1
max.budget <- 1

# Use the maq package to fit a Qini curve, using 200 bootstrap replicates for SEs.
qini <- maq(tau.hat, cost, max.budget, Y.ipw.test, R = 200)

# Form a baseline Qini curve that uses the ATE rather than the CATE to assign treatment.
qini.baseline <- maq(tau.hat, cost, max.budget, Y.ipw.test, target.with.covariates = FALSE)

plot(qini)
plot(qini.baseline, add = TRUE, lty = 2)
```

The solid curve shows the expected gain (y-axis) as we assign treatment to units predicted to benefit the most per unit spent, as we increase the amount we are willing to spend per unit (x-axis). The straight dashed line shows the Qini curve when we assign treatment without considering the CATE, i.e., at the end of the x-axis, at which point we have exhausted the budget and given everyone the treatment, our gain is equal to the ATE of around `r mean(Y.ipw.test)`. (So, points on the dashed-line represent the fraction of the ATE we can expect when targeting an arbitrary group of the population at different spend levels, thus it does not have to be a 45-degree line)

The solid black curve is the Qini curve that uses the estimated CATEs to predict which test set subjects have the highest treatment effect per unit spent. As this curve rises sharply above the dashed straight line that “ignores” the CATE, it suggests there is a benefit to targeting treatment to a subpopulation as implied by the estimated CATEs, that is most responsive per unit spent.

We can read off estimates of the curve at various $B$ through:

```{r}
average_gain(qini, spend = 0.2)
```

*Note on policy evaluation:* whenever IPW can solve a problem, there is generally a doubly robust method (here Augmented-IPW) that can do better in terms of statistical power. In this vignette, we'll stick to evaluating with IPW for simplicity, but note that with GRF could you train a separate test set forest and retrieve doubly robust test set scores through the function `get_scores(forest)` that could be used in place of `Y.ipw.test`.

### Aside: The Qini curve vs the TOC curve
We have thus far introduced *two curves* that may seem to both serve a similar purpose. What's the difference? That depends on the application. If we look closer, which of these curves are useful depends on what questions we are interested in. Below is a side-by-side illustration of what the [TOC](https://grf-labs.github.io/grf/articles/rate.html#quantifying-treatment-benefit-the-targeting-operator-characteristic-1) and Qini curves could look like
```{r, echo=FALSE}
n <- 1000
p <- 5
# training set
X.train <- matrix(rnorm(n * p), n, p)
W.train <- rbinom(n, 1, 0.5)
Y.train <- pmax(X.train[, 1], 0) * W.train + X.train[, 2] + pmin(X.train[, 3], 0) + rnorm(n)
cf.train <- causal_forest(X.train, Y.train, W.train, num.trees = 500)

# test set
X.test <- matrix(rnorm(n * p), n, p)
W.test <- rbinom(n, 1, 0.5)
Y.test <- pmax(X.test[, 1], 0) * W.test + X.test[, 2] + pmin(X.test[, 3], 0) + rnorm(n)

# evaluation
cate.hat.test <- predict(cf.train, X.test)$predictions
cf.test <- causal_forest(X.test, Y.test, W.test, num.trees = 500)
DR.test <- get_scores(cf.test)

# TOC
rate <- rank_average_treatment_effect.fit(DR.test, cate.hat.test)

# Qini
cost <- 1
qini <- maq(cate.hat.test, cost, 10, DR.test, R = 200)
qini.ate <- maq(cate.hat.test, cost, 10, DR.test, target.with.covariates = FALSE)

par(mfrow = c(1, 2))
plot(rate, sub = "", xlab = "Treated fraction", main = "TOC curve")

plot(qini, ylab = "", xlab = "Spend", main = "Qini curve")
plot(qini.ate, add = TRUE, lty = 2)
```

The TOC curve is useful if you are in a setting where you are interested in whether there is a group of people that benefit more than average from being treated. If costs are not directly relevant, and you are more interested in detecting a presence of heterogeneity, then the TOC is helpful because it clearly reveals the quantile where targeting helps. The TOC thus serves as a general tool for measuring heterogeneity.

The Qini curve is useful if you are in a setting where it is natural to undertake some cost-benefit analysis. The Qini curve is helpful because it clearly reveals the expected gain you can achieve by targeting the most responsive units, per spend, at various levels of budget allocations available. Another benefit of the Qini curve is that it is also more natural to be extended to many treatment arms, as the cost element carries over a similar cost-benefit tradeoff, but now across both arms, and units.

*Note on terminology:* We call the area under the TOC curve a RATE metric. This metric depends on a weighting method. "AUTOC" is the RATE when we place higher weight on units with higher estimated CATEs. "QINI" is the RATE when we give each unit the same weight. For details see Yadlowsky et al. (2021)


## Qini curves with multi-armed treatment
Consider now the case where we have $k = 0,\ldots K$ arms available, where $k=0$ is a zero cost control arm. For example, $k=1$ could be a low cost drug, and $k=2$ could be a higher cost drug, but which is more effective (and $k=0$ could be a placebo control).

Given estimated treatment effects $\hat \tau(\cdot)$ and costs $\hat C(\cdot)$ (remember that these objects are now vector-valued, i.e. the $k$-th element of $\hat \tau(X_i)$ is an estimate of the CATE for arm $k$ for units with covariates $X_i$) - we now ask: how can we conduct a similar exercise as above where we evaluate allocating treatment optimally in accordance with our estimated CATEs (and costs), as we vary the available budget?

It turns out that in order to perform this exercise, we need to solve a constrained optimization problem, as the underlying policy object $\hat \pi_B(X_i)$ now has to optimally select among many potential arms (with different costs) for each unit. For example, at each spend level, we have to decide whether we should allocate some *initial* arm to Alice, *or* perhaps, if Bob was already an assigned arm, if we instead should *upgrade* Bob to a costlier, but more effective arm.

The [maq](https://github.com/grf-labs/maq) package performs this exercise efficiently, and we’ll here jump straight into a toy example with 2 treatment arms and a control. We are simulating a simple example where one treatment arm (number 2) is more effective but costlier, and the costs vary by unit based on some known characteristic:
```{r}
# Simulate two treatment arms (1 and 2) and a control arm 0.
n <- 3000
p <- 5
X <- matrix(runif(n * p), n, p)
W <- as.factor(sample(c("0", "1", "2"), n, replace = TRUE))
Y <- X[, 1] + X[, 2] * (W == "1") + 1.5 * X[, 3] * (W == "2") + rnorm(n)

# 1) Train a CATE function on training set.
train <- sample(n/2)
c.forest <- multi_arm_causal_forest(X[train, ], Y[train], W[train])

# 2) Predict CATEs on test set.
test <- -train
tau.hat <- predict(c.forest, X[test, ], drop = TRUE)$predictions

# 3) Form a multi-armed Qini curve based on IPW.
observed.W <- match(W, levels(W))
Y.mat <- matrix(0, length(W), nlevels(W))
Y.mat[cbind(seq_along(observed.W), observed.W)] <- Y
Y.ipw <- sweep(Y.mat, 2, rep(1/3, 3), "/")
Y.ipw.test <- Y.ipw[test, -1] - Y.ipw[test, 1]

# The cost of arm 1 and 2
cost <- cbind(X[test, 4] / 4, X[test, 5])
max.budget <- 2

ma.qini <- maq(tau.hat, cost, max.budget, Y.ipw.test, R = 200)
plot(ma.qini)
```

We can also compute single Qini curves for each treatment arm and overlay them in the plot:
```{r}
qini.arm1 <- maq(tau.hat[, 1], cost[, 1], max.budget, Y.ipw.test[, 1], R = 200)
qini.arm2 <- maq(tau.hat[, 2], cost[, 2], max.budget, Y.ipw.test[, 2], R = 200)

plot(ma.qini, ci.args = NULL) # leave out CIs for legibility.
plot(qini.arm1, add = TRUE, col = "blue", ci.args = NULL)
plot(qini.arm2, add = TRUE, col = "red", ci.args = NULL)
legend("topleft", c("Qini (both arms)", "Qini (arm 1)", "Qini (arm 2)"),
       col = c("black", "blue", "red"), lty = 1)
```

The blue line (arm 1) plateaus at a spend level of around `r max(qini.arm1[["_path"]]$spend)`, since once we have reached this spend level, we are already giving treatment to all units believed to benefit from arm 1, and so cannot achieve further gains via increased spending.

The multi-armed Qini generalization allows us to answer questions such as "For a specific spend level, what is the estimated increase in gain when optimally targeting with both arms as opposed to using only a single arm?" For example, at $B = 0.1$ this difference over arm 1 would be would be
```{r}
difference_gain(ma.qini, qini.arm1, spend = 0.1)
```

We can retrieve the underlying estimated multi-armed policy through `predict`, for example at $B=0.1$:
```{r}
pi.hat <- predict(ma.qini, spend = 0.1)
head(pi.hat)
```
$\hat \pi_B(X_i)$ is now a $K$-dimensional vector, i.e. the $i$-th row above has a 1 in the $k$-th column if we at that spend level assign arm $k$ to that unit, with all entries zero if the control arm is assigned. (Note that at most one unit will have a non-integer entry in this row matrix. If the particular budget is not enough to give unit $i$ the treatment, then this unit will either have a single fractional entry, or two fractional entries, depending on whether the unit is being assigned an initial arm or upgraded to a costlier arm. These fractions can be interpreted as a probability of arm assignment.)

The mean treatment fractions are
```{r}
colMeans(pi.hat)
```
i.e., at this spend level assigning `r round(100*mean(pi.hat[,1]), 0)` % of the units to arm 1 and `r round(100*mean(pi.hat[,2]), 0)` % of the units to arm 2 (and `r round(100 - 100*sum(colMeans(pi.hat)), 0)` % to the control) is optimal.

Finally, we can also construct and plot a “baseline” policy that optimally selects among the two treatment arms based on only the ATE:
```{r}
ma.qini.basline <- maq(tau.hat, cost, max.budget, Y.ipw.test, target.with.covariates = FALSE, R = 200)

plot(ma.qini, ci.args = NULL)
plot(qini.arm1, add = TRUE, col = "blue", ci.args = NULL)
plot(qini.arm2, add = TRUE, col = "red", ci.args = NULL)
plot(ma.qini.basline, add = TRUE, lty = 2, ci.args = NULL)
legend("topleft", c("Qini (both arms)", "Qini (arm 1)", "Qini (arm 2)"),
       col = c("black", "blue", "red"), lty = 1)
```


We can conduct similar hypotheses tests as above, now assessing the value of optimally targeting based on covariates, against just allocating arbitrary units based on the arm ATEs:
```{r}
difference_gain(ma.qini, ma.qini.basline, spend = 0.1)
```

For more details, including a central limit theorem covering the standard errors above, we refer to the [paper](https://arxiv.org/abs/2306.11979).


## Appendix: The underlying optimization problem
To give an idea of the problem `maq` is solving under the hood, for a test set $i=1\ldots n$ with CATE estimates $\hat \tau(X_i)$ and costs $\widehat C(X_i)$ and a spend level $B$, the multi-armed policy is the solution to the following linear program,

$$
\begin{aligned}
\max_{\pi_B} \quad & \sum_{i=1}^{n} \langle \pi_B(X_i),~ \hat \tau(X_i) \rangle \\
\textrm{s.t.} \quad & \sum_{i=1}^{n} \langle \pi_B(X_i),~ \widehat C(X_i) \rangle \leq B \\
& \langle \pi_B(X_i),~ \mathbf{1} \rangle \leq 1 \\
& \pi_B(X_i) \geq 0.
\end{aligned}
$$

It is possible to solve this using off-the-shelf convex solvers, such as for example `lpSolve`. Note that solving for $\hat \pi_B$ involves repeatedly invoking an LP solver with different budget constraints, one for each point on the spend curve. This would quickly get computationally infeasible, particularly for large $n$. `maq` computes the entire *solution path* to the above linear program for all $B$ constraints, for details see Algorithm 2 in the [paper](https://arxiv.org/abs/2306.11979).


## References
Sun, Hao, Evan Munro, Georgy Kalashnov, Shuyang Du, and Stefan Wager. Treatment Allocation under Uncertain Costs. _arXiv preprint arXiv:2103.11066_ ([arxiv](https://arxiv.org/abs/2103.11066v3))

Sverdrup, Erik, Han Wu, Susan Athey, and Stefan Wager. Qini Curves for Multi-Armed Treatment Rules. _arXiv preprint arXiv:2306.11979_ ([arxiv](https://arxiv.org/abs/2306.11979))

Yadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager. Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects. _arXiv preprint arXiv:2111.07966_ ([arxiv](https://arxiv.org/abs/2111.07966))


[^c]: In the case of a binary treatment, when costs are unknown and have to be estimated, there turns out to be a more efficient approach to forming estimates of the cost-benefit ratio than estimating CATEs and costs separately. In some settings one may achieve an efficiency gain by instead targeting the ratio directly. This can be done through an instrumental variable formulation, as described in Sun et al. (2021).
